#!/usr/bin/env python
from __future__ import absolute_import

import argparse
import glob
import logging
import os
import shutil
import subprocess
import sys
import tarfile
import zipfile

try:
    from urllib.parse import quote
except ImportError:
    from urllib import quote  # noqa

from pip.backwardcompat import ConfigParser
from pip.download import _download_url, _get_response_from_url
from pip.index import PackageFinder
from pip.locations import default_config_file
from pip.req import InstallRequirement

from piptools.datastructures import Spec, SpecSet, FileSource


DEFAULT_REQUIREMENTS_FILE = 'requirements.in'
GLOB_PATTERN = '*requirements.in'


def setup_logging(verbose):
    if verbose:
        level = logging.DEBUG
    else:
        level = logging.INFO

    logging.basicConfig(level=level, format='%(message)s')


def parse_args():
    parser = argparse.ArgumentParser(
            description='Compiles requirements.txt from requirements.in specs.')
    parser.add_argument('--dry-run', action='store_true', default=False,
            help="Only show what would happen, don't change anything")
    parser.add_argument('--verbose', '-v', action='store_true', default=False,
            help='Show more output')
    parser.add_argument('files', nargs='*')
    return parser.parse_args()


def get_pip_cache_root():
    """Returns pip's cache root, or None if no such cache root is configured."""
    pip_config = ConfigParser.RawConfigParser()
    pip_config.read([default_config_file])
    download_cache = None
    try:
        for key, value in pip_config.items('global'):
            if key == 'download-cache':
                download_cache = value
                break
    except ConfigParser.NoSectionError:
        pass
    if download_cache is not None:
        download_cache = os.path.expanduser(download_cache)
    return download_cache


def resolve_deps(filename):
    name, ext = os.path.splitext(filename)
    if not ext == '.in':
        raise ValueError("Top-level requirements should be named "
                         "*requirements.in")
    with open(filename, 'r') as f:
        reqs = f.read()
    download_dir = os.path.join(os.path.expanduser('~'),
                                '.pip-tools', 'cache')

    if not os.path.isdir(download_dir):
        os.makedirs(download_dir)

    deps = {}

    for line in reqs.splitlines():
        resolve_dep(line, deps)

    reqs = "\n".join(sorted(deps.keys()))
    with open('{}.txt'.format(name), 'w') as f:
        f.write(reqs + '\n')


def extract_archive(path, build_dir):
    if (path.endswith('.tar.gz') or
        path.endswith('.tar') or
        path.endswith('.tar.bz2') or
        path.endswith('.tgz')):

        archive = tarfile.open(path)
    elif path.endswith('.zip'):
        archive = zipfile.ZipFile(path)
    else:
        assert False, "Unsupported archive file: {}".format(path)

    archive.extractall(build_dir)
    archive.close()


def has_egg_info(name, dist_dir):
    try:
        subprocess.check_call([sys.executable, 'setup.py', 'egg_info'],
                              cwd=dist_dir, stdout=subprocess.PIPE,
                              stderr=subprocess.PIPE)
    except subprocess.CalledProcessError:
        logging.debug("egg_info failed for {}".format(name))
        return False
    return True


def read_requires(dist_dir, name):
    egg_info_dir = '{0}.egg-info'.format(name.replace('-', '_'))
    for dirpath, dirnames, filenames in os.walk(dist_dir):
        if egg_info_dir in dirnames:
            requires = os.path.join(dirpath, egg_info_dir, 'requires.txt')
            if not os.path.exists(requires):
                logging.debug("{} has no declared dependencies".format(name))
                break
            return requires


def resolve_dep(line, deps):
    logging.info("Resolving {}".format(line))
    path = download_requirement(line)

    build_dir = os.path.join(os.path.dirname(path), 'build')
    if os.path.exists(build_dir):
        shutil.rmtree(build_dir)

    extract_archive(path, build_dir)

    name = os.listdir(build_dir)[0]
    dist_dir = os.path.join(build_dir, name)
    name, version = name.rsplit('-', 1)
    deps['{0}=={1}'.format(name, version)] = path

    if not has_egg_info(name, dist_dir):
        return

    requires = read_requires(dist_dir, name)
    if not requires:
        return

    with open(requires, 'r') as requirements:
        for dep in requirements.readlines():
            dep = dep.strip()
            if dep == '[test]' or not dep:
                break
            resolve_dep(dep, deps)


def download_requirement(line):
    req = InstallRequirement.from_line(line)
    finder = PackageFinder(
        find_links=[],
        index_urls=['http://pypi.python.org/simple/'],
        use_mirrors=True,
        mirrors=[],
    )
    found = finder.find_requirement(req, False)

    download_dir = os.path.join(os.path.expanduser('~'),
                                '.pip-tools', 'cache')
    filename = quote(found.url.split('#')[0], '')
    final_path = os.path.join(download_dir, filename)
    if os.path.exists(final_path):
        # requirement already cached
        return final_path

    pip_cache_root = get_pip_cache_root()
    if pip_cache_root:
        cache_path = os.path.join(pip_cache_root, filename)
        if os.path.exists(cache_path):
            # pip has a cached version, copy it
            shutil.copyfile(cache_path, final_path)

    else:
        # actually download the requirement
        response = _get_response_from_url(found.url.split('#')[0], found)
        _download_url(response, found, final_path)
    return final_path


def walk_specfile(filename):
    """Walks over the given file, and returns (req, filename, lineno)
    tuples for each entry.
    """
    with open(filename, 'r') as f:
        reqs = f.read()

    for lineno, line in enumerate(reqs.splitlines(), 1):
        line = line.strip()
        if not line or line.startswith('#'):
            continue

        spec = Spec.from_line(line)
        spec.source = FileSource(filename, lineno)
        yield spec


def collect_source_specs(filenames):
    """This function collects all of the (primary) source specs into
    a flattened list of specs.
    """
    logging.debug('===> Collecting source requirements')
    for filename in filenames:
        for spec in walk_specfile(filename):
            logging.debug('%s: %s' % (spec.source, spec))
            yield spec


def compile_specs(source_files, dry_run=False):
    top_level_specs = collect_source_specs(source_files)

    spec_set = SpecSet()
    spec_set.add_specs(top_level_specs)

    print spec_set  # normalizes the spec set and prints the resulting set

    # TODO: Implement the rest
    # We have the normalized spec_set for the top-level dependencies.
    #
    # Still to be done:
    # - For the normalized spec set, try to find packages using the
    #   PackageFinder.  If not found, the spec can't be compiled.
    # - Get a list of next-level dependency Specs (this will mean some
    #   download, unarchive, inspect work)
    # - Add those new specs to the SpecSet, and normalize() again.
    # - Repeat until no new specs are added.


def main():
    args = parse_args()
    setup_logging(args.verbose)

    src_files = args.files or glob.glob(GLOB_PATTERN)
    compile_specs(src_files, dry_run=args.dry_run)

    if args.dry_run:
        logging.info("Dry-run, so nothing updated.")
    else:
        logging.info("Dependencies updated.")


if __name__ == '__main__':
    main()
